# Project Requirements

- Minimal locally runnable self-improving LLM for modest hardware.
- Compact transformer (10-50M parameters) with a GPT2-like config.
- Utilize PyTorch and Hugging Face Transformers.
- Implement a self-improvement loop: candidate generation, critic evaluation, simple RL update.
- Prioritize clean, efficient code (files under 500 lines). 